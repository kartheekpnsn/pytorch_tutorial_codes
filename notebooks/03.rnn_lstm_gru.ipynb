{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c3bedbf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T10:07:38.590003Z",
     "start_time": "2023-05-09T10:07:37.907670Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    # torch.backends.cudnn.deterministic = True\n",
    "    # torch.backends.cudnn.benchmark = False\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    print(f\"Random seed set as {seed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d96c75c",
   "metadata": {},
   "source": [
    "## RNN - In built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61455e8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T10:08:20.009722Z",
     "start_time": "2023-05-09T10:07:38.592499Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 42\n",
      "Device used: cpu\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "Epoch: 1/2, Step: 100/600, Loss: 0.9081\n",
      "Epoch: 1/2, Step: 200/600, Loss: 0.3941\n",
      "Epoch: 1/2, Step: 300/600, Loss: 0.4813\n",
      "Epoch: 1/2, Step: 400/600, Loss: 0.3429\n",
      "Epoch: 1/2, Step: 500/600, Loss: 0.3256\n",
      "Epoch: 1/2, Step: 600/600, Loss: 0.3805\n",
      "Epoch: 2/2, Step: 100/600, Loss: 0.5672\n",
      "Epoch: 2/2, Step: 200/600, Loss: 0.2217\n",
      "Epoch: 2/2, Step: 300/600, Loss: 0.2479\n",
      "Epoch: 2/2, Step: 400/600, Loss: 0.2086\n",
      "Epoch: 2/2, Step: 500/600, Loss: 0.2300\n",
      "Epoch: 2/2, Step: 600/600, Loss: 0.2075\n",
      "Accuracy of the network on the 10000 test images: 94.89 %\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# device config\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device used: {device}\")\n",
    "\n",
    "# hyperparameters\n",
    "# input_size = 784  # 28 x 28\n",
    "num_classes = 10\n",
    "n_epochs = 2\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "input_size = 28\n",
    "sequence_length = 28\n",
    "hidden_size = 128\n",
    "num_layers = 2 # Stacking 2 RNNs\n",
    "\n",
    "# load data\n",
    "train_dataset = torchvision.datasets.MNIST(root=\"../data\",\n",
    "                                           train=True,\n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=False)\n",
    "test_dataset = torchvision.datasets.MNIST(root=\"../data\",\n",
    "                                          train=False,\n",
    "                                          transform=transforms.ToTensor(),\n",
    "                                          download=False)\n",
    "\n",
    "# data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "\n",
    "# plot samples\n",
    "examples = iter(train_loader)\n",
    "samples, labels = next(examples)\n",
    "print(samples.shape, labels.shape)\n",
    "\n",
    "for i in range(6):\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    plt.imshow(samples[i][0], cmap=\"gray\")  # [0] for first channel\n",
    "\n",
    "\n",
    "# define model\n",
    "class RNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes=10):\n",
    "        super(RNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        # x -> (batch_size, seq, input_size)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes) \n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "        out, _ = self.rnn(x, h0) # size -> (batch_size, seq_length, hidden_size), (N, 28, 128)\n",
    "        # What we want is (N, 128)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "# define loss & optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# training loop\n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(n_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # reshape images from 100 x 1 x 28 x 28 --> 100 x 28 x 28\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(\n",
    "                f\"Epoch: {epoch + 1}/{n_epochs}, Step: {i+1}/{n_total_steps}, Loss: {loss.item():.4f}\"\n",
    "            )\n",
    "\n",
    "# evaluation\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for images, labels in test_loader:\n",
    "        # reshape images from 100 x 1 x 28 x 28 --> 100 x 28 x 28\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "\n",
    "        # returns value, index and we are interested in \"index\"\n",
    "        _, predictions = torch.max(outputs, axis=1)\n",
    "        n_samples = n_samples + labels.size(0)\n",
    "\n",
    "        n_correct += (predictions == labels).sum().item()\n",
    "\n",
    "    acc = 100.0 * (n_correct / n_samples)\n",
    "    print(f'Accuracy of the network on the 10000 test images: {acc} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcef750",
   "metadata": {},
   "source": [
    "## LSTM - Inbuilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "187dd966",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T10:10:20.840225Z",
     "start_time": "2023-05-09T10:08:20.011301Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 42\n",
      "Device used: cpu\n",
      "Epoch: 1/2, Step: 100/600, Loss: 0.6078\n",
      "Epoch: 1/2, Step: 200/600, Loss: 0.4355\n",
      "Epoch: 1/2, Step: 300/600, Loss: 0.2759\n",
      "Epoch: 1/2, Step: 400/600, Loss: 0.2150\n",
      "Epoch: 1/2, Step: 500/600, Loss: 0.2349\n",
      "Epoch: 1/2, Step: 600/600, Loss: 0.1109\n",
      "Epoch: 2/2, Step: 100/600, Loss: 0.1330\n",
      "Epoch: 2/2, Step: 200/600, Loss: 0.0426\n",
      "Epoch: 2/2, Step: 300/600, Loss: 0.0903\n",
      "Epoch: 2/2, Step: 400/600, Loss: 0.1464\n",
      "Epoch: 2/2, Step: 500/600, Loss: 0.1373\n",
      "Epoch: 2/2, Step: 600/600, Loss: 0.1599\n",
      "Accuracy of the network on the 10000 test images: 97.18 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# device config\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device used: {device}\")\n",
    "\n",
    "# hyperparameters\n",
    "# input_size = 784  # 28 x 28\n",
    "num_classes = 10\n",
    "n_epochs = 2\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "input_size = 28\n",
    "sequence_length = 28\n",
    "hidden_size = 128\n",
    "num_layers = 2 # Stacking 2 RNNs\n",
    "\n",
    "# load data\n",
    "train_dataset = torchvision.datasets.MNIST(root=\"../data\",\n",
    "                                           train=True,\n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=False)\n",
    "test_dataset = torchvision.datasets.MNIST(root=\"../data\",\n",
    "                                          train=False,\n",
    "                                          transform=transforms.ToTensor(),\n",
    "                                          download=False)\n",
    "\n",
    "# data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "\n",
    "\n",
    "# define model\n",
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes=10):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        # x -> (batch_size, seq, input_size)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes) \n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0)) # size -> (batch_size, seq_length, hidden_size), (N, 28, 128)\n",
    "        # What we want is (N, 128)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = LSTM(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "# define loss & optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# training loop\n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(n_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # reshape images from 100 x 1 x 28 x 28 --> 100 x 28 x 28\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(\n",
    "                f\"Epoch: {epoch + 1}/{n_epochs}, Step: {i+1}/{n_total_steps}, Loss: {loss.item():.4f}\"\n",
    "            )\n",
    "\n",
    "# evaluation\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for images, labels in test_loader:\n",
    "        # reshape images from 100 x 1 x 28 x 28 --> 100 x 28 x 28\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "\n",
    "        # returns value, index and we are interested in \"index\"\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        n_samples = n_samples + labels.size(0)\n",
    "\n",
    "        n_correct += (predictions == labels).sum().item()\n",
    "\n",
    "    acc = 100.0 * (n_correct / n_samples)\n",
    "    print(f'Accuracy of the network on the 10000 test images: {acc} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e087e1",
   "metadata": {},
   "source": [
    "## GRU - Inbuilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01c669ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T10:12:05.319298Z",
     "start_time": "2023-05-09T10:10:20.841836Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 42\n",
      "Device used: cpu\n",
      "Epoch: 1/2, Step: 100/600, Loss: 0.8453\n",
      "Epoch: 1/2, Step: 200/600, Loss: 0.3421\n",
      "Epoch: 1/2, Step: 300/600, Loss: 0.2726\n",
      "Epoch: 1/2, Step: 400/600, Loss: 0.1081\n",
      "Epoch: 1/2, Step: 500/600, Loss: 0.1390\n",
      "Epoch: 1/2, Step: 600/600, Loss: 0.1607\n",
      "Epoch: 2/2, Step: 100/600, Loss: 0.1424\n",
      "Epoch: 2/2, Step: 200/600, Loss: 0.1008\n",
      "Epoch: 2/2, Step: 300/600, Loss: 0.1171\n",
      "Epoch: 2/2, Step: 400/600, Loss: 0.1928\n",
      "Epoch: 2/2, Step: 500/600, Loss: 0.0602\n",
      "Epoch: 2/2, Step: 600/600, Loss: 0.1883\n",
      "Accuracy of the network on the 10000 test images: 97.44 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# device config\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device used: {device}\")\n",
    "\n",
    "# hyperparameters\n",
    "# input_size = 784  # 28 x 28\n",
    "num_classes = 10\n",
    "n_epochs = 2\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "input_size = 28\n",
    "sequence_length = 28\n",
    "hidden_size = 128\n",
    "num_layers = 2 # Stacking 2 RNNs\n",
    "\n",
    "# load data\n",
    "train_dataset = torchvision.datasets.MNIST(root=\"../data\",\n",
    "                                           train=True,\n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=False)\n",
    "test_dataset = torchvision.datasets.MNIST(root=\"../data\",\n",
    "                                          train=False,\n",
    "                                          transform=transforms.ToTensor(),\n",
    "                                          download=False)\n",
    "\n",
    "# data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "# define model\n",
    "class GRU(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes=10):\n",
    "        super(GRU, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        # x -> (batch_size, seq, input_size)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes) \n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "        out, _ = self.gru(x, h0) # size -> (batch_size, seq_length, hidden_size), (N, 28, 128)\n",
    "        # What we want is (N, 128)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = GRU(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "# define loss & optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# training loop\n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(n_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # reshape images from 100 x 1 x 28 x 28 --> 100 x 28 x 28\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(\n",
    "                f\"Epoch: {epoch + 1}/{n_epochs}, Step: {i+1}/{n_total_steps}, Loss: {loss.item():.4f}\"\n",
    "            )\n",
    "\n",
    "# evaluation\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for images, labels in test_loader:\n",
    "        # reshape images from 100 x 1 x 28 x 28 --> 100 x 28 x 28\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "\n",
    "        # returns value, index and we are interested in \"index\"\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        n_samples = n_samples + labels.size(0)\n",
    "\n",
    "        n_correct += (predictions == labels).sum().item()\n",
    "\n",
    "    acc = 100.0 * (n_correct / n_samples)\n",
    "    print(f'Accuracy of the network on the 10000 test images: {acc} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
